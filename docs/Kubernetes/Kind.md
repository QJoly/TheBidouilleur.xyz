---
slug: kind
title: Kubernetes INside Docker
description: Avoir un environnement de d√©veloppement est important dans tous les domaines. L'administration d'un Cluster Kubernetes n'√©chappe pas √† la r√®gle. Nous allons donc voir comment cr√©er un cluster temporaire directement √† partir de conteneurs Docker.
---

## Introduction

Lorsque l‚Äôon d√©veloppe un chart, ou que l‚Äôon souhaite faire des tests sur Kubernetes *(ex¬†: rbac)*, nous n‚Äôallons pas faire ces tests sur un environnement de production. Nous n‚Äôavons non-plus pas toujours de cluster d√©di√© aux tests. 
Alors avec l‚Äôaide de **KIND** : nous allons pouvoir cr√©er ce cluster de test, et sans devoir installer la moindre machine virtuelle. 

Comme son nom l‚Äôindique ce soft permet de cr√©er un cluster √† partir de **conteneurs Docker**. C‚Äôest d‚Äôailleurs ce que j‚Äôutilise dans le CI pr√©sent√© ici : [Cr√©er un d√©pot Helm](/blog/Creer-son-registre-helm)

:::info
Ma configuration : 
- Docker  : 20.10.22
- Kind    : 0.17.0
- Linux   : Ubuntu amd64 (6.0.12)
:::


## Pr√©-requis si vous utilisez Docker en RootLess


:::caution Docker en Rootless
Vous pouvez passer cette partie si vous n‚Äôutilisez pas Docker en Rootless. 
:::

V√©rifiez si les *cgroup v2* sont activ√©s sur votre poste : 
```bash
‚ûú grep cgroup /proc/filesystems
nodev	cgroup
nodev	cgroup2
```

Si ce n‚Äôest pas le cas, vous devrez ajouter `GRUB_CMDLINE_LINUX="systemd.unified_cgroup_hierarchy=1"` √† votre fichier `/etc/default/grub`, puis mettre √† jour votre grub : `sudo update-grub`. 

Lancez ensuite un cluster kind : 
```bash
‚ûú kind create cluster      
ERROR: failed to create cluster: running kind with rootless provider requires setting systemd property "Delegate=yes", see https://kind.sigs.k8s.io/docs/user/rootless/
```
Si vous tombez sur la m√™me erreur que moi, alors suivez les prochaines √©tapes. *(sinon on se retrouve un peu plus bas)*

```bash
sudo mkdir -p /etc/systemd/system/user@.service.d
sudo sh -c 'cat >/etc/systemd/system/user@.service.d/delegate.conf <<EOF
[Service]
Delegate=yes
EOF'
sudo sh -c 'cat >>/etc/modules-load.d/iptables.conf <<EOF
ip6_tables
ip6table_nat
ip_tables
iptable_nat
EOF'
sudo systemctl daemon-reload
```

Nous retentons apr√®s coup : 
```bash
‚ûú kind create cluster         
Creating cluster "kind" ...
 ‚úì Ensuring node image (kindest/node:v1.25.3) üñº 
 ‚úì Preparing nodes üì¶  
 ‚úì Writing configuration üìú 
 ‚úì Starting control-plane üïπÔ∏è 
 ‚úì Installing CNI üîå 
 ‚úì Installing StorageClass üíæ 
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Have a nice day! üëã
‚ûú kubectl cluster-info --context kind-kind
Kubernetes control plane is running at https://127.0.0.1:45975
CoreDNS is running at https://127.0.0.1:45975/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

Pour le supprimer : `kind delete cluster kind-kind`

:::caution Des limitations d‚Äôutiliser Docker en rootless
- Les montagnes *OverlayFS* ne peuvent √™tre activ√© que si votre kernel est *>= 5.11*
- Impossible de mount les *block storages* *(iSCSI, Aws, Local Volume)*
- Impossible de mount un partage *NFS*
:::

:::tip Kubectx
Je vous conseille d‚Äôutiliser *Kubectx* pour changer votre context utilis√©.
```
kubectx kind-kind
```
:::

Les conteneurs ‚Äònodes‚Äô sont visibles directement avec la *cli* Docker. 
```bash
‚ûú  docker ps
CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS          PORTS                      NAMES
288826f42d30   kindest/node:v1.25.3   "/usr/local/bin/entr‚Ä¶"   14 hours ago   Up 41 minutes   127.0.0.1:6443->6443/tcp   test-control-plane
98738fa957e4   kindest/node:v1.25.3   "/usr/local/bin/entr‚Ä¶"   14 hours ago   Up 41 minutes                              test-worker
```

## Configuration KIND

Si l‚Äôusage de KIND est assez simple, il est toujours possible de rajouter des petits param√®tres pour l‚Äôadapter √† nos besoins. 

Par exemple, il est possible de choisir le r√©seau des pods/services. Il suffit de cr√©er un fichier yaml qui accueillera notre configuration :

```
# kind-config.yml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: reseau-en-192
networking:
  podSubnet: "192.168.244.0/24"
  serviceSubnet: "192.168.137.0/24"
```

Et nous cr√©ons ce cluster √† partir de notre fichier `kind-config.yml`.
```bash
‚ûú kind create cluster --config=kind-config.yml
```
Et nous observons les IPs choisies par notre cluster: 
```bash
‚ûú  kubectl get pods -A -o wide
NAMESPACE            NAME                                                  READY   STATUS    RESTARTS   AGE   IP              NODE                          NOMINATED NODE   READINESS GATES
kube-system          coredns-565d847f94-pmgqf                              1/1     Running   0          28s   192.168.244.2   reseau-en-192-control-plane   <none>           <none>
kube-system          coredns-565d847f94-rd5jh                              1/1     Running   0          28s   192.168.244.3   reseau-en-192-control-plane   <none>           <none>
kube-system          etcd-reseau-en-192-control-plane                      1/1     Running   0          42s   172.18.0.2      reseau-en-192-control-plane   <none>           <none>
kube-system          kindnet-4bhxr                                         1/1     Running   0          28s   172.18.0.2      reseau-en-192-control-plane   <none>           <none>
kube-system          kube-apiserver-reseau-en-192-control-plane            1/1     Running   0          42s   172.18.0.2      reseau-en-192-control-plane   <none>           <none>
kube-system          kube-controller-manager-reseau-en-192-control-plane   1/1     Running   0          42s   172.18.0.2      reseau-en-192-control-plane   <none>           <none>
kube-system          kube-proxy-g88wr                                      1/1     Running   0          28s   172.18.0.2      reseau-en-192-control-plane   <none>           <none>
kube-system          kube-scheduler-reseau-en-192-control-plane            1/1     Running   0          43s   172.18.0.2      reseau-en-192-control-plane   <none>           <none>
local-path-storage   local-path-provisioner-684f458cdd-bdnmt               1/1     Running   0          28s   192.168.244.4   reseau-en-192-control-plane   <none>           <none>
‚ûú  kubectl get service -A -o wide
NAMESPACE     NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE   SELECTOR
default       kubernetes   ClusterIP   192.168.137.1    <none>        443/TCP                  75s   <none>
kube-system   kube-dns     ClusterIP   192.168.137.10   <none>        53/UDP,53/TCP,9153/TCP   74s   k8s-app=kube-dns
```
Nous avons bien nos pods sur le r√©seau 192.168.244.0/24, et les services sur 192.168.137.0/24. 

:::info R√©seau en 172.18.0.0/16
Ce r√©seau est g√©r√© par Docker, Kind n‚Äôa pas la main-mise sur le choix des IPs. 
C‚Äôest un r√©seau Docker ind√©pendant des autres conteneurs. 
Pour voir sa configuration : `docker network inspect kind`
:::

Si l‚Äôon souhaite avoir un second noeud, il suffit de mettre l‚Äôinstruction : 
```yaml
[...]
nodes:
- role: control-plane
- role: worker
```
On peut √©galement ajouter des labels aux nodes :

```yml
[...]
- role: worker
  labels:
    role: db-controler
```

Les possibilit√©s sont vastes, la documentation de KIND est tr√®s claire et montre de nombreux cas d‚Äôutilisations. Voici le fichier que j‚Äôutilise au quotidien dans mes diff√©rents tests : 

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: mon-petit-cluster
networking:
  apiServerAddress: "127.0.0.1"
  apiServerPort: 6443
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 3000
    hostPort: 3000
    listenAddress: "127.0.0.1"
    protocol: TCP
- role: worker
  labels:
    tier: backend
```

